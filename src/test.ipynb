{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "print('Hello World!')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before weights\n",
      "[[1, 4, 3, 2, 5], [1, 4, 3, 2, 5], [1, 4, 3, 2, 5]]\n",
      "[[3, 2, 1, 4], [3, 2, 1, 4], [3, 2, 1, 4]]\n",
      "2\n",
      "24\n",
      "-60\n",
      "After weights\n",
      "[[1, 4, 3, 2, 5], [1, 4, 3, 2, 5], [1, 4, 3, 2, 5]]\n",
      "[[3, 2, 1, 4], [3, 2, 1, 4], [3, 2, 1, 4]]\n"
     ]
    }
   ],
   "source": [
    "# 4x3x3 NN\n",
    "# weights = [[[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]], [[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]]]\n",
    "\n",
    "weights = [[[1, 4, 3, 2, 5], [1, 4, 3, 2, 5], [1, 4, 3, 2, 5]], [[3, 2, 1, 4], [3, 2, 1, 4], [3, 2, 1, 4]]]\n",
    "x = [[6.0, 2.2, 4.0, 1.0, 1], [0.10, 0.72, 0.31, 1], [0.65, 0.44, 0.40]]\n",
    "\n",
    "# weights = [[[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 3.8, 5]], [[1, 2, 2.8, 4], [1, 2, 2.6, 4], [1, 2, 3.4, 4]]]\n",
    "\n",
    "# weights = [[[1, 3.8, 3, 4, 5], [1, 2, 3, 4, 5], [1, 4, 3, 2, 5]], [[2.8, 2, 1, 4], [2.6, 2, 1, 4], [3.4, 2, 1, 4]]]\n",
    "input = [1, 1, 1, 1]\n",
    "output = [2, 4, 6]\n",
    "target = [1, 2, 8]\n",
    "\n",
    "print('Before weights')\n",
    "for w in weights:\n",
    "    print(w)\n",
    "\n",
    "refreshNeuralPathways(output, target)\n",
    "\n",
    "print('After weights')\n",
    "for w in weights:\n",
    "    print(w)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# weight_update = copy.deepcopy(weights)\n",
    "def refreshNeuralPathways(output, target):\n",
    "    # utils.log('Training w rpr', None )\n",
    "    eta = 0.2\n",
    "    updates_ref = []\n",
    "    updates = []\n",
    "    units = x[1:] #Exclude input layer\n",
    "    \n",
    "    for i, x in enumerate(zip(output, target)):\n",
    "        o = x[0]\n",
    "        t = x[1]\n",
    "        # err = t-o #Use different loss func if needed \n",
    "        delta = o*(1-o)*(t-o)\n",
    "        # print(delta)\n",
    "\n",
    "        \"\"\"\n",
    "            unit_input_activations = a_reversed[layer] #From this unit's persepective looking at activations from the prev layers that feed into it\n",
    "            a_reversed includes output activations, should we not skip the very first one (last one before you reverse)\n",
    "        \"\"\"\n",
    "        curr_conn_id = i\n",
    "        for layer in range(len(weights)-1, -1, -1): #Loop backwards\n",
    "            # print(layer)\n",
    "            # o_k = units[layer][curr_hop_id] #Activation of unit at desired index\n",
    "            # delta_out = o_k*(1-o_k)*(t-o_k)\n",
    "            \n",
    "            unit_weights = weights[layer][curr_conn_id] \n",
    "            # utils.log('unit_input_activations', unit_input_activations)\n",
    "            #Get the strongest weight (Weight most responsible for the err)\n",
    "            strongest_weight = max(unit_weights[:-1]) #Ignore last weight because it is bias (TODO: thinks about how else you want to handle bias weights - maybe just ignoring them isn't the best for best performance)\n",
    "            next_conn_id = unit_weights.index(strongest_weight) \n",
    "            # unit_weights[next_hop_id] += err #update strongest  \n",
    "            # w_reversed[layer][curr_hop_id][next_hop_id] += self.eta*err*unit_input_activations[next_hop_id] #Try w and w/o unit_input_activation (xji  in T4.5 from the book)\n",
    "            #TODO: Calculate delta, use delta for this hidden unit to udpdate weights\n",
    "            sensitivity = strongest_weight*delta #Sensitivity of strongest weight\n",
    "            o_h = units[next_conn_id] #Hidden unit that stronges weight leads to\n",
    "            delta = o_h*(1-o_h)*sensitivity #GD\n",
    "            # weight_update[layer][curr_conn_id][next_conn_id] = eta*delta*units[next_conn_id] #Store the update\n",
    "            updates.append(eta*delta*units[next_conn_id]) #Store the update\n",
    "            updates_ref.append([layer, curr_conn_id, next_conn_id]) #Store references of weights to udpates\n",
    "            # unit_weights[next_hop_id] += err #update strongest  \n",
    "            # w_reversed[layer][curr_hop_id][next_hop_id] += self.eta*err*unit_input_activations[next_hop_id] #Try w and w/o unit_input_activation (xji  in T4.5 from the book)\n",
    "            # weights[layer][curr_hop_id][next_hop_id] += eta*delta*units[next_hop_id]\n",
    "            curr_conn_id = next_conn_id\n",
    "\n",
    "    #Apply updates (only to weights of ids)\n",
    "    for update, update_ref in zip(updates, updates_ref):\n",
    "        weights[update_ref[0]][update_ref[1]][update_ref[2]] += update\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38740d3277777e2cd7c6c2cc9d8addf5118fdf3f82b1b39231fd12aeac8aee8b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
